{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from model import  ResNetGenerator, Discriminator\n",
    "from data import *\n",
    "from utils import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epochs = 20\n",
    "epoch = 0\n",
    "decay_epoch = 10\n",
    "sample_interval = 100\n",
    "dataset_name = \"horse2zebra\"\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "channels = 3\n",
    "\n",
    "input_shape = (channels, img_height, img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU State: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "print('GPU State:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if device else torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image transformations\n",
    "data_process_steps = [\n",
    "    transforms.Resize((100,100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "# gender\n",
    "# |-testA\n",
    "# |-testB\n",
    "# |-trainA\n",
    "# |-trainB\n",
    "\n",
    "train_data = DataLoader(\n",
    "    ImageDataset(\n",
    "        \"C:/Users/a8701/NTUST_dissertation/dataset/horse2zebra/horse2zebra\",\n",
    "        transforms_=data_process_steps,\n",
    "        unaligned=True,\n",
    "    ),\n",
    "    batch_size=1,\n",
    "    #shuffle=True,\n",
    "    num_workers=4,\n",
    " )\n",
    "\n",
    "\n",
    "test_data = DataLoader(\n",
    "    ImageDataset(\n",
    "        \"C:/Users/a8701/NTUST_dissertation/dataset/horse2zebra/horse2zebra\",\n",
    "        transforms_=data_process_steps,\n",
    "        unaligned=True,\n",
    "        mode=\"test\",\n",
    "    ),\n",
    "    batch_size=5,\n",
    "    #shuffle=True,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "Gen_AB = ResNetGenerator(input_shape).to(device)\n",
    "Gen_BA = ResNetGenerator(input_shape).to(device)\n",
    "Dis_A = Discriminator(input_shape).to(device)\n",
    "Dis_B = Discriminator(input_shape).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "opt_G = torch.optim.Adam(\n",
    "    itertools.chain(Gen_AB.parameters(),Gen_BA.parameters()),\n",
    "    lr=0.00001\n",
    ")\n",
    "opt_D_A = torch.optim.Adam(\n",
    "    Dis_A.parameters(),\n",
    "    lr = 0.00001\n",
    ")\n",
    "\n",
    "opt_D_B = torch.optim.Adam(\n",
    "    Dis_B.parameters(),\n",
    "    lr = 0.00001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    opt_G, lr_lambda=LambdaLR(epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    opt_D_A, lr_lambda=LambdaLR(epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    opt_D_B, lr_lambda=LambdaLR(epochs, epoch, decay_epoch).step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(test_data))\n",
    "    Gen_AB.eval()\n",
    "    Gen_BA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = Gen_AB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = Gen_BA(real_B)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
    "    save_image(image_grid, \"images/%s/%s.png\" % (dataset_name, batches_done), normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ss: 0.0365, cycle loss: 0.3434, idenity: 0.3363\n",
      "Epoch: 0/20, Batch: 348/1334, D loss: 0.0425, G loss: 4.0701, adv loss: 0.0443, cycle loss: 0.2722, idenity: 0.2607\n",
      "Epoch: 0/20, Batch: 349/1334, D loss: 0.0459, G loss: 4.4046, adv loss: 0.0405, cycle loss: 0.2934, idenity: 0.2861\n",
      "Epoch: 0/20, Batch: 350/1334, D loss: 0.0444, G loss: 4.5683, adv loss: 0.0399, cycle loss: 0.3051, idenity: 0.2955\n",
      "Epoch: 0/20, Batch: 351/1334, D loss: 0.0421, G loss: 3.6492, adv loss: 0.0313, cycle loss: 0.2433, idenity: 0.2370\n",
      "Epoch: 0/20, Batch: 352/1334, D loss: 0.0352, G loss: 3.1232, adv loss: 0.0289, cycle loss: 0.2118, idenity: 0.1952\n",
      "Epoch: 0/20, Batch: 353/1334, D loss: 0.0398, G loss: 3.9746, adv loss: 0.0401, cycle loss: 0.2657, idenity: 0.2555\n",
      "Epoch: 0/20, Batch: 354/1334, D loss: 0.0391, G loss: 5.1348, adv loss: 0.0333, cycle loss: 0.3457, idenity: 0.3289\n",
      "Epoch: 0/20, Batch: 355/1334, D loss: 0.0371, G loss: 3.5788, adv loss: 0.0344, cycle loss: 0.2405, idenity: 0.2279\n",
      "Epoch: 0/20, Batch: 356/1334, D loss: 0.0363, G loss: 2.9465, adv loss: 0.0369, cycle loss: 0.1964, idenity: 0.1891\n",
      "Epoch: 0/20, Batch: 357/1334, D loss: 0.0316, G loss: 3.4656, adv loss: 0.0312, cycle loss: 0.2333, idenity: 0.2203\n",
      "Epoch: 0/20, Batch: 358/1334, D loss: 0.0533, G loss: 5.0437, adv loss: 0.0368, cycle loss: 0.3358, idenity: 0.3297\n",
      "Epoch: 0/20, Batch: 359/1334, D loss: 0.0399, G loss: 3.8603, adv loss: 0.0355, cycle loss: 0.2581, idenity: 0.2488\n",
      "Epoch: 0/20, Batch: 360/1334, D loss: 0.0270, G loss: 3.6049, adv loss: 0.0280, cycle loss: 0.2420, idenity: 0.2314\n",
      "Epoch: 0/20, Batch: 361/1334, D loss: 0.0453, G loss: 3.6294, adv loss: 0.0372, cycle loss: 0.2430, idenity: 0.2324\n",
      "Epoch: 0/20, Batch: 362/1334, D loss: 0.0345, G loss: 5.8264, adv loss: 0.0321, cycle loss: 0.3926, idenity: 0.3736\n",
      "Epoch: 0/20, Batch: 363/1334, D loss: 0.0371, G loss: 3.7561, adv loss: 0.0314, cycle loss: 0.2503, idenity: 0.2444\n",
      "Epoch: 0/20, Batch: 364/1334, D loss: 0.0202, G loss: 4.1661, adv loss: 0.0186, cycle loss: 0.2806, idenity: 0.2683\n",
      "Epoch: 0/20, Batch: 365/1334, D loss: 0.0371, G loss: 3.7125, adv loss: 0.0227, cycle loss: 0.2496, idenity: 0.2388\n",
      "Epoch: 0/20, Batch: 366/1334, D loss: 0.0514, G loss: 4.2077, adv loss: 0.0422, cycle loss: 0.2801, idenity: 0.2729\n",
      "Epoch: 0/20, Batch: 367/1334, D loss: 0.0351, G loss: 4.5143, adv loss: 0.0298, cycle loss: 0.3027, idenity: 0.2914\n",
      "Epoch: 0/20, Batch: 368/1334, D loss: 0.0379, G loss: 4.6779, adv loss: 0.0395, cycle loss: 0.3135, idenity: 0.3006\n",
      "Epoch: 0/20, Batch: 369/1334, D loss: 0.0446, G loss: 4.2375, adv loss: 0.0430, cycle loss: 0.2839, idenity: 0.2712\n",
      "Epoch: 0/20, Batch: 370/1334, D loss: 0.0393, G loss: 3.4420, adv loss: 0.0349, cycle loss: 0.2279, idenity: 0.2256\n",
      "Epoch: 0/20, Batch: 371/1334, D loss: 0.0389, G loss: 3.5445, adv loss: 0.0330, cycle loss: 0.2370, idenity: 0.2284\n",
      "Epoch: 0/20, Batch: 372/1334, D loss: 0.0289, G loss: 4.4714, adv loss: 0.0283, cycle loss: 0.3037, idenity: 0.2812\n",
      "Epoch: 0/20, Batch: 373/1334, D loss: 0.0305, G loss: 3.1362, adv loss: 0.0273, cycle loss: 0.2085, idenity: 0.2049\n",
      "Epoch: 0/20, Batch: 374/1334, D loss: 0.0389, G loss: 3.6756, adv loss: 0.0348, cycle loss: 0.2453, idenity: 0.2376\n",
      "Epoch: 0/20, Batch: 375/1334, D loss: 0.0340, G loss: 4.1940, adv loss: 0.0374, cycle loss: 0.2780, idenity: 0.2752\n",
      "Epoch: 0/20, Batch: 376/1334, D loss: 0.0393, G loss: 4.1683, adv loss: 0.0322, cycle loss: 0.2803, idenity: 0.2667\n",
      "Epoch: 0/20, Batch: 377/1334, D loss: 0.0407, G loss: 3.5660, adv loss: 0.0385, cycle loss: 0.2377, idenity: 0.2302\n",
      "Epoch: 0/20, Batch: 378/1334, D loss: 0.0361, G loss: 3.6205, adv loss: 0.0273, cycle loss: 0.2454, idenity: 0.2278\n",
      "Epoch: 0/20, Batch: 379/1334, D loss: 0.0350, G loss: 3.5062, adv loss: 0.0424, cycle loss: 0.2359, idenity: 0.2209\n",
      "Epoch: 0/20, Batch: 380/1334, D loss: 0.0346, G loss: 6.8395, adv loss: 0.0226, cycle loss: 0.4555, idenity: 0.4524\n",
      "Epoch: 0/20, Batch: 381/1334, D loss: 0.0386, G loss: 4.2198, adv loss: 0.0347, cycle loss: 0.2838, idenity: 0.2693\n",
      "Epoch: 0/20, Batch: 382/1334, D loss: 0.0378, G loss: 3.9747, adv loss: 0.0348, cycle loss: 0.2702, idenity: 0.2475\n",
      "Epoch: 0/20, Batch: 383/1334, D loss: 0.0381, G loss: 5.1561, adv loss: 0.0332, cycle loss: 0.3432, idenity: 0.3383\n",
      "Epoch: 0/20, Batch: 384/1334, D loss: 0.0414, G loss: 4.2598, adv loss: 0.0277, cycle loss: 0.2851, idenity: 0.2762\n",
      "Epoch: 0/20, Batch: 385/1334, D loss: 0.0365, G loss: 3.8096, adv loss: 0.0243, cycle loss: 0.2562, idenity: 0.2447\n",
      "Epoch: 0/20, Batch: 386/1334, D loss: 0.0312, G loss: 4.5125, adv loss: 0.0236, cycle loss: 0.3021, idenity: 0.2935\n",
      "Epoch: 0/20, Batch: 387/1334, D loss: 0.0317, G loss: 4.2170, adv loss: 0.0353, cycle loss: 0.2811, idenity: 0.2742\n",
      "Epoch: 0/20, Batch: 388/1334, D loss: 0.0279, G loss: 4.0340, adv loss: 0.0254, cycle loss: 0.2730, idenity: 0.2557\n",
      "Epoch: 0/20, Batch: 389/1334, D loss: 0.0372, G loss: 4.0244, adv loss: 0.0311, cycle loss: 0.2690, idenity: 0.2607\n",
      "Epoch: 0/20, Batch: 390/1334, D loss: 0.0333, G loss: 3.5705, adv loss: 0.0314, cycle loss: 0.2360, idenity: 0.2358\n",
      "Epoch: 0/20, Batch: 391/1334, D loss: 0.0334, G loss: 3.9574, adv loss: 0.0308, cycle loss: 0.2693, idenity: 0.2467\n",
      "Epoch: 0/20, Batch: 392/1334, D loss: 0.0345, G loss: 4.3342, adv loss: 0.0317, cycle loss: 0.2905, idenity: 0.2794\n",
      "Epoch: 0/20, Batch: 393/1334, D loss: 0.0373, G loss: 3.2524, adv loss: 0.0400, cycle loss: 0.2183, idenity: 0.2059\n",
      "Epoch: 0/20, Batch: 394/1334, D loss: 0.0315, G loss: 2.7881, adv loss: 0.0243, cycle loss: 0.1874, idenity: 0.1779\n",
      "Epoch: 0/20, Batch: 395/1334, D loss: 0.0442, G loss: 3.0105, adv loss: 0.0387, cycle loss: 0.2001, idenity: 0.1942\n",
      "Epoch: 0/20, Batch: 396/1334, D loss: 0.0443, G loss: 3.6919, adv loss: 0.0363, cycle loss: 0.2442, idenity: 0.2427\n",
      "Epoch: 0/20, Batch: 397/1334, D loss: 0.0316, G loss: 4.1815, adv loss: 0.0296, cycle loss: 0.2782, idenity: 0.2740\n",
      "Epoch: 0/20, Batch: 398/1334, D loss: 0.0310, G loss: 4.6306, adv loss: 0.0354, cycle loss: 0.3100, idenity: 0.2991\n",
      "Epoch: 0/20, Batch: 399/1334, D loss: 0.0252, G loss: 4.3769, adv loss: 0.0229, cycle loss: 0.2936, idenity: 0.2837\n",
      "Epoch: 0/20, Batch: 400/1334, D loss: 0.0205, G loss: 3.5061, adv loss: 0.0172, cycle loss: 0.2351, idenity: 0.2276\n",
      "Epoch: 0/20, Batch: 401/1334, D loss: 0.0348, G loss: 4.5775, adv loss: 0.0325, cycle loss: 0.3037, idenity: 0.3016\n",
      "Epoch: 0/20, Batch: 402/1334, D loss: 0.0345, G loss: 3.7625, adv loss: 0.0213, cycle loss: 0.2519, idenity: 0.2444\n",
      "Epoch: 0/20, Batch: 403/1334, D loss: 0.0341, G loss: 3.9543, adv loss: 0.0414, cycle loss: 0.2647, idenity: 0.2532\n",
      "Epoch: 0/20, Batch: 404/1334, D loss: 0.0387, G loss: 4.0011, adv loss: 0.0224, cycle loss: 0.2675, idenity: 0.2607\n",
      "Epoch: 0/20, Batch: 405/1334, D loss: 0.0381, G loss: 3.7180, adv loss: 0.0395, cycle loss: 0.2459, idenity: 0.2438\n",
      "Epoch: 0/20, Batch: 406/1334, D loss: 0.0284, G loss: 3.0747, adv loss: 0.0282, cycle loss: 0.2036, idenity: 0.2020\n",
      "Epoch: 0/20, Batch: 407/1334, D loss: 0.0374, G loss: 3.9952, adv loss: 0.0382, cycle loss: 0.2658, idenity: 0.2599\n",
      "Epoch: 0/20, Batch: 408/1334, D loss: 0.0276, G loss: 5.3315, adv loss: 0.0300, cycle loss: 0.3558, idenity: 0.3487\n",
      "Epoch: 0/20, Batch: 409/1334, D loss: 0.0262, G loss: 4.1114, adv loss: 0.0262, cycle loss: 0.2754, idenity: 0.2662\n",
      "Epoch: 0/20, Batch: 410/1334, D loss: 0.0331, G loss: 3.8294, adv loss: 0.0301, cycle loss: 0.2567, idenity: 0.2464\n",
      "Epoch: 0/20, Batch: 411/1334, D loss: 0.0286, G loss: 3.5771, adv loss: 0.0268, cycle loss: 0.2402, idenity: 0.2296\n",
      "Epoch: 0/20, Batch: 412/1334, D loss: 0.0274, G loss: 4.5669, adv loss: 0.0257, cycle loss: 0.3047, idenity: 0.2988\n",
      "Epoch: 0/20, Batch: 413/1334, D loss: 0.0343, G loss: 4.1731, adv loss: 0.0299, cycle loss: 0.2790, idenity: 0.2706\n",
      "Epoch: 0/20, Batch: 414/1334, D loss: 0.0379, G loss: 3.3018, adv loss: 0.0325, cycle loss: 0.2225, idenity: 0.2088\n",
      "Epoch: 0/20, Batch: 415/1334, D loss: 0.0285, G loss: 3.4524, adv loss: 0.0317, cycle loss: 0.2303, idenity: 0.2236\n",
      "Epoch: 0/20, Batch: 416/1334, D loss: 0.0284, G loss: 4.2132, adv loss: 0.0322, cycle loss: 0.2831, idenity: 0.2700\n",
      "Epoch: 0/20, Batch: 417/1334, D loss: 0.0393, G loss: 4.5082, adv loss: 0.0209, cycle loss: 0.3025, idenity: 0.2924\n",
      "Epoch: 0/20, Batch: 418/1334, D loss: 0.0312, G loss: 3.1121, adv loss: 0.0231, cycle loss: 0.2091, idenity: 0.1996\n",
      "Epoch: 0/20, Batch: 419/1334, D loss: 0.0320, G loss: 4.7992, adv loss: 0.0266, cycle loss: 0.3199, idenity: 0.3148\n",
      "Epoch: 0/20, Batch: 420/1334, D loss: 0.0446, G loss: 4.7080, adv loss: 0.0307, cycle loss: 0.3125, idenity: 0.3105\n",
      "Epoch: 0/20, Batch: 421/1334, D loss: 0.0396, G loss: 3.7194, adv loss: 0.0376, cycle loss: 0.2483, idenity: 0.2397\n",
      "Epoch: 0/20, Batch: 422/1334, D loss: 0.0352, G loss: 3.6474, adv loss: 0.0283, cycle loss: 0.2427, idenity: 0.2384\n",
      "Epoch: 0/20, Batch: 423/1334, D loss: 0.0289, G loss: 4.0864, adv loss: 0.0268, cycle loss: 0.2747, idenity: 0.2626\n",
      "Epoch: 0/20, Batch: 424/1334, D loss: 0.0388, G loss: 2.8957, adv loss: 0.0274, cycle loss: 0.1959, idenity: 0.1819\n",
      "Epoch: 0/20, Batch: 425/1334, D loss: 0.0277, G loss: 5.8166, adv loss: 0.0283, cycle loss: 0.3870, idenity: 0.3837\n",
      "Epoch: 0/20, Batch: 426/1334, D loss: 0.0281, G loss: 3.2621, adv loss: 0.0282, cycle loss: 0.2196, idenity: 0.2076\n",
      "Epoch: 0/20, Batch: 427/1334, D loss: 0.0341, G loss: 4.9361, adv loss: 0.0232, cycle loss: 0.3297, idenity: 0.3232\n",
      "Epoch: 0/20, Batch: 428/1334, D loss: 0.0400, G loss: 4.0461, adv loss: 0.0318, cycle loss: 0.2742, idenity: 0.2545\n",
      "Epoch: 0/20, Batch: 429/1334, D loss: 0.0344, G loss: 5.1334, adv loss: 0.0287, cycle loss: 0.3429, idenity: 0.3351\n",
      "Epoch: 0/20, Batch: 430/1334, D loss: 0.0376, G loss: 5.4460, adv loss: 0.0248, cycle loss: 0.3679, idenity: 0.3485\n",
      "Epoch: 0/20, Batch: 431/1334, D loss: 0.0425, G loss: 4.3136, adv loss: 0.0320, cycle loss: 0.2890, idenity: 0.2783\n",
      "Epoch: 0/20, Batch: 432/1334, D loss: 0.0286, G loss: 4.1534, adv loss: 0.0242, cycle loss: 0.2800, idenity: 0.2659\n",
      "Epoch: 0/20, Batch: 433/1334, D loss: 0.0358, G loss: 3.9100, adv loss: 0.0239, cycle loss: 0.2616, idenity: 0.2539\n",
      "Epoch: 0/20, Batch: 434/1334, D loss: 0.0317, G loss: 3.2690, adv loss: 0.0210, cycle loss: 0.2183, idenity: 0.2130\n",
      "Epoch: 0/20, Batch: 435/1334, D loss: 0.0360, G loss: 4.7812, adv loss: 0.0223, cycle loss: 0.3199, idenity: 0.3119\n",
      "Epoch: 0/20, Batch: 436/1334, D loss: 0.0322, G loss: 4.2030, adv loss: 0.0242, cycle loss: 0.2802, idenity: 0.2754\n",
      "Epoch: 0/20, Batch: 437/1334, D loss: 0.0286, G loss: 4.2959, adv loss: 0.0227, cycle loss: 0.2868, idenity: 0.2809\n",
      "Epoch: 0/20, Batch: 438/1334, D loss: 0.0275, G loss: 4.1912, adv loss: 0.0263, cycle loss: 0.2799, idenity: 0.2731\n",
      "Epoch: 0/20, Batch: 439/1334, D loss: 0.0332, G loss: 3.8490, adv loss: 0.0282, cycle loss: 0.2560, idenity: 0.2522\n",
      "Epoch: 0/20, Batch: 440/1334, D loss: 0.0275, G loss: 3.9227, adv loss: 0.0324, cycle loss: 0.2624, idenity: 0.2533\n",
      "Epoch: 0/20, Batch: 441/1334, D loss: 0.0360, G loss: 3.7975, adv loss: 0.0348, cycle loss: 0.2537, idenity: 0.2452\n",
      "Epoch: 0/20, Batch: 442/1334, D loss: 0.0181, G loss: 3.5031, adv loss: 0.0193, cycle loss: 0.2355, idenity: 0.2258\n",
      "Epoch: 0/20, Batch: 443/1334, D loss: 0.0333, G loss: 5.1858, adv loss: 0.0403, cycle loss: 0.3432, idenity: 0.3427\n",
      "Epoch: 0/20, Batch: 444/1334, D loss: 0.0281, G loss: 3.9739, adv loss: 0.0176, cycle loss: 0.2684, idenity: 0.2545\n",
      "Epoch: 0/20, Batch: 445/1334, D loss: 0.0423, G loss: 4.2554, adv loss: 0.0314, cycle loss: 0.2834, idenity: 0.2779\n",
      "Epoch: 0/20, Batch: 446/1334, D loss: 0.0283, G loss: 4.3487, adv loss: 0.0184, cycle loss: 0.2926, idenity: 0.2810\n",
      "Epoch: 0/20, Batch: 447/1334, D loss: 0.0341, G loss: 3.2842, adv loss: 0.0199, cycle loss: 0.2225, idenity: 0.2078\n",
      "Epoch: 0/20, Batch: 448/1334, D loss: 0.0238, G loss: 3.6111, adv loss: 0.0240, cycle loss: 0.2414, idenity: 0.2346\n",
      "Epoch: 0/20, Batch: 449/1334, D loss: 0.0310, G loss: 4.5595, adv loss: 0.0295, cycle loss: 0.3032, idenity: 0.2996\n",
      "Epoch: 0/20, Batch: 450/1334, D loss: 0.0304, G loss: 5.3579, adv loss: 0.0228, cycle loss: 0.3561, idenity: 0.3548\n",
      "Epoch: 0/20, Batch: 451/1334, D loss: 0.0277, G loss: 3.8983, adv loss: 0.0189, cycle loss: 0.2647, idenity: 0.2466\n",
      "Epoch: 0/20, Batch: 452/1334, D loss: 0.0362, G loss: 4.1908, adv loss: 0.0298, cycle loss: 0.2794, idenity: 0.2734\n",
      "Epoch: 0/20, Batch: 453/1334, D loss: 0.0282, G loss: 3.8561, adv loss: 0.0211, cycle loss: 0.2579, idenity: 0.2512\n",
      "Epoch: 0/20, Batch: 454/1334, D loss: 0.0361, G loss: 4.3718, adv loss: 0.0309, cycle loss: 0.2916, idenity: 0.2851\n",
      "Epoch: 0/20, Batch: 455/1334, D loss: 0.0387, G loss: 3.5367, adv loss: 0.0317, cycle loss: 0.2378, idenity: 0.2254\n",
      "Epoch: 0/20, Batch: 456/1334, D loss: 0.0385, G loss: 3.7904, adv loss: 0.0384, cycle loss: 0.2532, idenity: 0.2440\n",
      "Epoch: 0/20, Batch: 457/1334, D loss: 0.0388, G loss: 4.6520, adv loss: 0.0282, cycle loss: 0.3094, idenity: 0.3060\n",
      "Epoch: 0/20, Batch: 458/1334, D loss: 0.0346, G loss: 3.0209, adv loss: 0.0265, cycle loss: 0.1997, idenity: 0.1995\n",
      "Epoch: 0/20, Batch: 459/1334, D loss: 0.0290, G loss: 3.2789, adv loss: 0.0249, cycle loss: 0.2178, idenity: 0.2153\n",
      "Epoch: 0/20, Batch: 460/1334, D loss: 0.0310, G loss: 4.3961, adv loss: 0.0322, cycle loss: 0.2904, idenity: 0.2919\n",
      "Epoch: 0/20, Batch: 461/1334, D loss: 0.0328, G loss: 4.1328, adv loss: 0.0252, cycle loss: 0.2771, idenity: 0.2674\n",
      "Epoch: 0/20, Batch: 462/1334, D loss: 0.0396, G loss: 5.0831, adv loss: 0.0317, cycle loss: 0.3424, idenity: 0.3255\n",
      "Epoch: 0/20, Batch: 463/1334, D loss: 0.0307, G loss: 3.9853, adv loss: 0.0283, cycle loss: 0.2659, idenity: 0.2597\n",
      "Epoch: 0/20, Batch: 464/1334, D loss: 0.0262, G loss: 3.5990, adv loss: 0.0232, cycle loss: 0.2402, idenity: 0.2348\n",
      "Epoch: 0/20, Batch: 465/1334, D loss: 0.0268, G loss: 4.3150, adv loss: 0.0255, cycle loss: 0.2908, idenity: 0.2764\n",
      "Epoch: 0/20, Batch: 466/1334, D loss: 0.0337, G loss: 3.8381, adv loss: 0.0374, cycle loss: 0.2562, idenity: 0.2478\n",
      "Epoch: 0/20, Batch: 467/1334, D loss: 0.0236, G loss: 5.2142, adv loss: 0.0286, cycle loss: 0.3521, idenity: 0.3328\n",
      "Epoch: 0/20, Batch: 468/1334, D loss: 0.0232, G loss: 3.6794, adv loss: 0.0219, cycle loss: 0.2463, idenity: 0.2389\n",
      "Epoch: 0/20, Batch: 469/1334, D loss: 0.0247, G loss: 3.1863, adv loss: 0.0285, cycle loss: 0.2145, idenity: 0.2026\n",
      "Epoch: 0/20, Batch: 470/1334, D loss: 0.0205, G loss: 3.8731, adv loss: 0.0206, cycle loss: 0.2561, idenity: 0.2583\n",
      "Epoch: 0/20, Batch: 471/1334, D loss: 0.0326, G loss: 2.6997, adv loss: 0.0283, cycle loss: 0.1798, idenity: 0.1746\n",
      "Epoch: 0/20, Batch: 472/1334, D loss: 0.0369, G loss: 4.8598, adv loss: 0.0269, cycle loss: 0.3255, idenity: 0.3156\n",
      "Epoch: 0/20, Batch: 473/1334, D loss: 0.0235, G loss: 5.2038, adv loss: 0.0178, cycle loss: 0.3491, idenity: 0.3390\n",
      "Epoch: 0/20, Batch: 474/1334, D loss: 0.0286, G loss: 4.3381, adv loss: 0.0252, cycle loss: 0.2901, idenity: 0.2824\n",
      "Epoch: 0/20, Batch: 475/1334, D loss: 0.0215, G loss: 3.2614, adv loss: 0.0198, cycle loss: 0.2197, idenity: 0.2088\n",
      "Epoch: 0/20, Batch: 476/1334, D loss: 0.0393, G loss: 3.7489, adv loss: 0.0348, cycle loss: 0.2529, idenity: 0.2371\n",
      "Epoch: 0/20, Batch: 477/1334, D loss: 0.0224, G loss: 4.0240, adv loss: 0.0202, cycle loss: 0.2689, idenity: 0.2630\n",
      "Epoch: 0/20, Batch: 478/1334, D loss: 0.0343, G loss: 4.1930, adv loss: 0.0299, cycle loss: 0.2810, idenity: 0.2706\n",
      "Epoch: 0/20, Batch: 479/1334, D loss: 0.0331, G loss: 4.1074, adv loss: 0.0216, cycle loss: 0.2763, idenity: 0.2645\n",
      "Epoch: 0/20, Batch: 480/1334, D loss: 0.0360, G loss: 3.4470, adv loss: 0.0257, cycle loss: 0.2322, idenity: 0.2199\n",
      "Epoch: 0/20, Batch: 481/1334, D loss: 0.0348, G loss: 3.2980, adv loss: 0.0256, cycle loss: 0.2212, idenity: 0.2120\n",
      "Epoch: 0/20, Batch: 482/1334, D loss: 0.0285, G loss: 4.2550, adv loss: 0.0243, cycle loss: 0.2850, idenity: 0.2762\n",
      "Epoch: 0/20, Batch: 483/1334, D loss: 0.0294, G loss: 4.5659, adv loss: 0.0221, cycle loss: 0.3051, idenity: 0.2986\n",
      "Epoch: 0/20, Batch: 484/1334, D loss: 0.0237, G loss: 4.3290, adv loss: 0.0280, cycle loss: 0.2886, idenity: 0.2829\n",
      "Epoch: 0/20, Batch: 485/1334, D loss: 0.0229, G loss: 4.2815, adv loss: 0.0240, cycle loss: 0.2866, idenity: 0.2783\n",
      "Epoch: 0/20, Batch: 486/1334, D loss: 0.0209, G loss: 2.6931, adv loss: 0.0200, cycle loss: 0.1810, idenity: 0.1726\n",
      "Epoch: 0/20, Batch: 487/1334, D loss: 0.0296, G loss: 4.4980, adv loss: 0.0204, cycle loss: 0.3005, idenity: 0.2944\n",
      "Epoch: 0/20, Batch: 488/1334, D loss: 0.0378, G loss: 5.0962, adv loss: 0.0245, cycle loss: 0.3369, idenity: 0.3405\n",
      "Epoch: 0/20, Batch: 489/1334, D loss: 0.0272, G loss: 4.8393, adv loss: 0.0195, cycle loss: 0.3217, idenity: 0.3205\n",
      "Epoch: 0/20, Batch: 490/1334, D loss: 0.0301, G loss: 4.7464, adv loss: 0.0245, cycle loss: 0.3160, idenity: 0.3125\n",
      "Epoch: 0/20, Batch: 491/1334, D loss: 0.0287, G loss: 3.3415, adv loss: 0.0279, cycle loss: 0.2264, idenity: 0.2100\n",
      "Epoch: 0/20, Batch: 492/1334, D loss: 0.0314, G loss: 4.7036, adv loss: 0.0267, cycle loss: 0.3164, idenity: 0.3025\n",
      "Epoch: 0/20, Batch: 493/1334, D loss: 0.0290, G loss: 4.4076, adv loss: 0.0241, cycle loss: 0.2979, idenity: 0.2809\n",
      "Epoch: 0/20, Batch: 494/1334, D loss: 0.0262, G loss: 3.8881, adv loss: 0.0216, cycle loss: 0.2604, idenity: 0.2525\n",
      "Epoch: 0/20, Batch: 495/1334, D loss: 0.0321, G loss: 3.9402, adv loss: 0.0282, cycle loss: 0.2648, idenity: 0.2528\n",
      "Epoch: 0/20, Batch: 496/1334, D loss: 0.0400, G loss: 3.1902, adv loss: 0.0276, cycle loss: 0.2149, idenity: 0.2027\n",
      "Epoch: 0/20, Batch: 497/1334, D loss: 0.0300, G loss: 3.2508, adv loss: 0.0287, cycle loss: 0.2199, idenity: 0.2046\n",
      "Epoch: 0/20, Batch: 498/1334, D loss: 0.0221, G loss: 3.5639, adv loss: 0.0196, cycle loss: 0.2394, idenity: 0.2301\n",
      "Epoch: 0/20, Batch: 499/1334, D loss: 0.0167, G loss: 4.1704, adv loss: 0.0191, cycle loss: 0.2813, idenity: 0.2676\n",
      "Epoch: 0/20, Batch: 500/1334, D loss: 0.0318, G loss: 6.5261, adv loss: 0.0263, cycle loss: 0.4382, idenity: 0.4236\n",
      "Epoch: 0/20, Batch: 501/1334, D loss: 0.0248, G loss: 3.6762, adv loss: 0.0210, cycle loss: 0.2470, idenity: 0.2371\n",
      "Epoch: 0/20, Batch: 502/1334, D loss: 0.0262, G loss: 4.4800, adv loss: 0.0151, cycle loss: 0.3023, idenity: 0.2884\n",
      "Epoch: 0/20, Batch: 503/1334, D loss: 0.0212, G loss: 4.0387, adv loss: 0.0213, cycle loss: 0.2672, idenity: 0.2691\n",
      "Epoch: 0/20, Batch: 504/1334, D loss: 0.0295, G loss: 3.9838, adv loss: 0.0284, cycle loss: 0.2668, idenity: 0.2574\n",
      "Epoch: 0/20, Batch: 505/1334, D loss: 0.0305, G loss: 3.4331, adv loss: 0.0267, cycle loss: 0.2297, idenity: 0.2220\n",
      "Epoch: 0/20, Batch: 506/1334, D loss: 0.0232, G loss: 3.8997, adv loss: 0.0224, cycle loss: 0.2566, idenity: 0.2623\n",
      "Epoch: 0/20, Batch: 507/1334, D loss: 0.0260, G loss: 3.9181, adv loss: 0.0266, cycle loss: 0.2641, idenity: 0.2501\n",
      "Epoch: 0/20, Batch: 508/1334, D loss: 0.0337, G loss: 4.3919, adv loss: 0.0381, cycle loss: 0.2928, idenity: 0.2851\n",
      "Epoch: 0/20, Batch: 509/1334, D loss: 0.0232, G loss: 4.1840, adv loss: 0.0204, cycle loss: 0.2794, idenity: 0.2740\n",
      "Epoch: 0/20, Batch: 510/1334, D loss: 0.0245, G loss: 3.4262, adv loss: 0.0240, cycle loss: 0.2328, idenity: 0.2148\n",
      "Epoch: 0/20, Batch: 511/1334, D loss: 0.0241, G loss: 3.0460, adv loss: 0.0245, cycle loss: 0.2032, idenity: 0.1980\n",
      "Epoch: 0/20, Batch: 512/1334, D loss: 0.0245, G loss: 4.0230, adv loss: 0.0206, cycle loss: 0.2724, idenity: 0.2557\n",
      "Epoch: 0/20, Batch: 513/1334, D loss: 0.0359, G loss: 3.9070, adv loss: 0.0220, cycle loss: 0.2566, idenity: 0.2639\n",
      "Epoch: 0/20, Batch: 514/1334, D loss: 0.0271, G loss: 3.6537, adv loss: 0.0276, cycle loss: 0.2445, idenity: 0.2362\n",
      "Epoch: 0/20, Batch: 515/1334, D loss: 0.0377, G loss: 3.9785, adv loss: 0.0306, cycle loss: 0.2664, idenity: 0.2567\n",
      "Epoch: 0/20, Batch: 516/1334, D loss: 0.0296, G loss: 4.4665, adv loss: 0.0228, cycle loss: 0.2994, idenity: 0.2900\n",
      "Epoch: 0/20, Batch: 517/1334, D loss: 0.0229, G loss: 3.4179, adv loss: 0.0145, cycle loss: 0.2262, idenity: 0.2283\n",
      "Epoch: 0/20, Batch: 518/1334, D loss: 0.0300, G loss: 3.6083, adv loss: 0.0261, cycle loss: 0.2421, idenity: 0.2323\n",
      "Epoch: 0/20, Batch: 519/1334, D loss: 0.0242, G loss: 3.1129, adv loss: 0.0152, cycle loss: 0.2086, idenity: 0.2023\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-05ffc1bf5cc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m#print(loss_G)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mloss_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[0mopt_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\a8701\\NTUST_dissertation\\deep-final\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\a8701\\NTUST_dissertation\\deep-final\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "for epoch in range(epochs):\n",
    "    for i, batch in enumerate(train_data):\n",
    "        \n",
    "        # Set model input\n",
    "        real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "        real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "        #print(real_A.shape)\n",
    "        #print(real_B.shape)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((real_A.size(0), *Dis_A.output_shape))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.ones((real_A.size(0), *Dis_A.output_shape))), requires_grad=False)\n",
    "\n",
    "        #print(valid.shape)\n",
    "        #print(fake.shape)\n",
    "\n",
    "        #\n",
    "        # Train Generators\n",
    "        #\n",
    "\n",
    "        Gen_AB.train()\n",
    "        Gen_BA.train()\n",
    "\n",
    "        opt_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        loss_id_A = criterion_identity(Gen_BA(real_A), real_A)\n",
    "        loss_id_B = criterion_identity(Gen_AB(real_B), real_B)\n",
    "\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "        #print(loss_identity)\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = Gen_AB(real_A)\n",
    "        #print(Dis_B(fake_B).shape)\n",
    "        loss_GAN_AB = criterion_GAN(Dis_B(fake_B), valid)\n",
    "        fake_A = Gen_BA(real_B)\n",
    "        loss_GAN_BA = criterion_GAN(Dis_A(fake_A), valid)\n",
    "\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "        # Cycle loss\n",
    "        recov_A = Gen_BA(fake_B)\n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "        recov_B = Gen_AB(fake_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + 10.0*loss_cycle + 5.0*loss_identity\n",
    "        #print(loss_G)\n",
    "\n",
    "        loss_G.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "        #\n",
    "        # Train Discriminator A\n",
    "        #\n",
    "\n",
    "        opt_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        loss_real = criterion_GAN(Dis_A(real_A), valid)\n",
    "\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "        loss_fake = criterion_GAN(Dis_A(fake_A_.detach()), fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_real + loss_fake) /2\n",
    "\n",
    "        loss_D_A.backward()\n",
    "        opt_D_A.step()\n",
    "\n",
    "        #\n",
    "        # Train Discrimainator B\n",
    "        #\n",
    "\n",
    "        opt_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        loss_real = criterion_GAN(Dis_B(real_B), valid)\n",
    "\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "        loss_fake = criterion_GAN(Dis_B(fake_B.detach()), fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D_B.backward()\n",
    "        opt_D_B.step()\n",
    "\n",
    "        loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "        #\n",
    "        # Log progress\n",
    "        #\n",
    "        batches_done = epoch * len(train_data) + i\n",
    "\n",
    "        print(\"Epoch: {}/{}, Batch: {}/{}, D loss: {:.4f}, G loss: {:.4f}, adv loss: {:.4f}, cycle loss: {:.4f}, idenity: {:.4f}\".format(epoch,epochs,i,len(train_data),loss_D.item(),loss_G.item(),loss_GAN.item(),loss_cycle.item(),loss_identity.item()))\n",
    "        \n",
    "        # If at sample interval save image\n",
    "        if batches_done % sample_interval == 0:\n",
    "            sample_images(batches_done)\n",
    "        \n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "    torch.save(Gen_AB.state_dict(), \"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch))\n",
    "    torch.save(Gen_BA.state_dict(), \"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch))\n",
    "    torch.save(Dis_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch))\n",
    "    torch.save(Dis_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}